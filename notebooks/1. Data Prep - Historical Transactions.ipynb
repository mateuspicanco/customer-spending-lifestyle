{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep - Credit Card Transactions\n",
    "This notebook documents my data processing steps done in `PySpark` in order to prepare the [Synthetic Credit Card Transactions](https://data.world/ealtman/synthetic-credit-card-transactions) dataset into a more appropriate format for modeling and data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing dependencies:\n",
    "!pip install -r ../configs/dependencies/dataprep_requirements.txt >> ../configs/dependencies/package_installation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Loading the necessary libraries #########\n",
    "\n",
    "# PySpark dependencies:s\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# other relevant libraries:\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "# trust me, this will make a lot of sense very soon\n",
    "import emoji\n",
    "\n",
    "# setting global parameters for visualizations:\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.2f\" % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Configuring Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the configurations needed for Spark\n",
    "def init_spark(app_name):\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(app_name)\n",
    "        .config(\"spark.files.overwrite\", \"true\")\n",
    "        .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "        .config(\"spark.sql.repl.eagerEval.maxNumRows\", 5)\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"gzip\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    return spark\n",
    "\n",
    "\n",
    "# init the spark session:\n",
    "spark = init_spark(\"Credit Card Transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.15.58:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Credit Card Transactions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb21a902ca0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying the session\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_filesystem(df, target_path, parquet_path, filename):\n",
    "    \"\"\"Helper function to save pyspark dataframes as parquets in a way that is similar to writing to local files\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.dataframe.DataFrame): dataframe to be saved\n",
    "        target_path (str): path that will store the file\n",
    "        filename (str): name of the resulting file\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    PARQUET_FILE = f\"{target_path}/{parquet_path}\"\n",
    "    OUTPUT_FILE = f\"{target_path}/{filename}\"\n",
    "\n",
    "    if os.path.exists(PARQUET_FILE):\n",
    "        shutil.rmtree(\n",
    "            PARQUET_FILE\n",
    "        )  # if the directory already exists, remove it (throws error if not)\n",
    "\n",
    "    # saves the dataframe:\n",
    "    df.coalesce(1).write.save(PARQUET_FILE)\n",
    "\n",
    "    # retrieves file resulting from the saving procedure:\n",
    "    original_file = glob(f\"{PARQUET_FILE}/*.parquet\")[0]\n",
    "\n",
    "    # renames the resulting file and saves it to the target directory:\n",
    "    os.rename(original_file, OUTPUT_FILE)\n",
    "\n",
    "    shutil.rmtree(PARQUET_FILE)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def apply_category_map(category_map):\n",
    "    \"\"\"Helper function to convert strings given a map\n",
    "\n",
    "    Note:\n",
    "        This function uses the function generator scheme, much like the PySpark code\n",
    "\n",
    "    Args:\n",
    "        original_category (str): the original category name\n",
    "        category_map (dict): the hash table or dictionary for converting the values:\n",
    "\n",
    "    Returns:\n",
    "        new_category (str): the resulting category\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def func(row):\n",
    "        try:\n",
    "            result = category_map[row]\n",
    "        except:\n",
    "            result = None\n",
    "        return result\n",
    "\n",
    "    return F.udf(func)\n",
    "\n",
    "\n",
    "def get_datetime_features(df, time_col):\n",
    "    \"\"\"Function to extract time-based features from pyspark dataframes\n",
    "\n",
    "    Args:\n",
    "        df (pyspark.sql.dataframe.DataFrame): the original dataframe that needs to be enriched\n",
    "        time_col (str): the string name of the column containing the date object\n",
    "\n",
    "    Returns:\n",
    "        df (pyspark.sql.dataframe.DataFrame): resulting pyspark dataframe with the added features\n",
    "            -> See list of attribute the source code for the attributes\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # applying date-related functions:\n",
    "\n",
    "    # day-level attributes:\n",
    "    df = df.withColumn(\"day_of_week\", F.dayofweek(F.col(time_col)))\n",
    "\n",
    "    df = df.withColumn(\"day_of_month\", F.dayofmonth(F.col(time_col)))\n",
    "\n",
    "    df = df.withColumn(\"day_of_year\", F.dayofyear(F.col(time_col)))\n",
    "\n",
    "    # week-level attributes:\n",
    "    df = df.withColumn(\"week_of_year\", F.weekofyear(F.col(time_col)))\n",
    "\n",
    "    # month-level attributes:\n",
    "    df = df.withColumn(\"month\", F.month(F.col(time_col)))\n",
    "\n",
    "    df = df.withColumn(\"quarter\", F.quarter(F.col(time_col)))\n",
    "\n",
    "    # year-level attributes:\n",
    "    df = df.withColumn(\"year\", F.year(F.col(time_col)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "@udf(T.StringType())\n",
    "def remove_duplicate_elements(sequence):\n",
    "    \"\"\"Helper UDF to remove consecutive duplicate elements in MCC sequences\"\"\"\n",
    "    return \"\".join(dict.fromkeys(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading and Inspecting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the raw dataset:\n",
    "RAW_DATA_DIR = \"../data/raw/\"\n",
    "\n",
    "# readng the training set raw data:\n",
    "df_transactions = spark.read.csv(RAW_DATA_DIR + \"card_transactions.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24386900"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting the number of records:\n",
    "df_transactions.count()  # thats a lot of records!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>User</th><th>Card</th><th>Year</th><th>Month</th><th>Day</th><th>Time</th><th>Amount</th><th>Use Chip</th><th>Merchant Name</th><th>Merchant City</th><th>Merchant State</th><th>Zip</th><th>MCC</th><th>Errors?</th><th>Is Fraud?</th></tr>\n",
       "<tr><td>0</td><td>0</td><td>2002</td><td>9</td><td>1</td><td>06:21</td><td>$134.09</td><td>Swipe Transaction</td><td>3527213246127876953</td><td>La Verne</td><td>CA</td><td>91750.0</td><td>5300</td><td>null</td><td>No</td></tr>\n",
       "<tr><td>0</td><td>0</td><td>2002</td><td>9</td><td>1</td><td>06:42</td><td>$38.48</td><td>Swipe Transaction</td><td>-727612092139916043</td><td>Monterey Park</td><td>CA</td><td>91754.0</td><td>5411</td><td>null</td><td>No</td></tr>\n",
       "<tr><td>0</td><td>0</td><td>2002</td><td>9</td><td>2</td><td>06:22</td><td>$120.34</td><td>Swipe Transaction</td><td>-727612092139916043</td><td>Monterey Park</td><td>CA</td><td>91754.0</td><td>5411</td><td>null</td><td>No</td></tr>\n",
       "<tr><td>0</td><td>0</td><td>2002</td><td>9</td><td>2</td><td>17:45</td><td>$128.95</td><td>Swipe Transaction</td><td>3414527459579106770</td><td>Monterey Park</td><td>CA</td><td>91754.0</td><td>5651</td><td>null</td><td>No</td></tr>\n",
       "<tr><td>0</td><td>0</td><td>2002</td><td>9</td><td>3</td><td>06:23</td><td>$104.71</td><td>Swipe Transaction</td><td>5817218446178736267</td><td>La Verne</td><td>CA</td><td>91750.0</td><td>5912</td><td>null</td><td>No</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+----+----+----+-----+---+-----+-------+-----------------+-------------------+-------------+--------------+-------+----+-------+---------+\n",
       "|User|Card|Year|Month|Day| Time| Amount|         Use Chip|      Merchant Name|Merchant City|Merchant State|    Zip| MCC|Errors?|Is Fraud?|\n",
       "+----+----+----+-----+---+-----+-------+-----------------+-------------------+-------------+--------------+-------+----+-------+---------+\n",
       "|   0|   0|2002|    9|  1|06:21|$134.09|Swipe Transaction|3527213246127876953|     La Verne|            CA|91750.0|5300|   null|       No|\n",
       "|   0|   0|2002|    9|  1|06:42| $38.48|Swipe Transaction|-727612092139916043|Monterey Park|            CA|91754.0|5411|   null|       No|\n",
       "|   0|   0|2002|    9|  2|06:22|$120.34|Swipe Transaction|-727612092139916043|Monterey Park|            CA|91754.0|5411|   null|       No|\n",
       "|   0|   0|2002|    9|  2|17:45|$128.95|Swipe Transaction|3414527459579106770|Monterey Park|            CA|91754.0|5651|   null|       No|\n",
       "|   0|   0|2002|    9|  3|06:23|$104.71|Swipe Transaction|5817218446178736267|     La Verne|            CA|91750.0|5912|   null|       No|\n",
       "+----+----+----+-----+---+-----+-------+-----------------+-------------------+-------------+--------------+-------+----+-------+---------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying a sample of the dataset:\n",
    "df_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User: string (nullable = true)\n",
      " |-- Card: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Amount: string (nullable = true)\n",
      " |-- Use Chip: string (nullable = true)\n",
      " |-- Merchant Name: string (nullable = true)\n",
      " |-- Merchant City: string (nullable = true)\n",
      " |-- Merchant State: string (nullable = true)\n",
      " |-- Zip: string (nullable = true)\n",
      " |-- MCC: string (nullable = true)\n",
      " |-- Errors?: string (nullable = true)\n",
      " |-- Is Fraud?: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# looking at the schema of the dataset:\n",
    "df_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataset, we can point out a few necessary steps for data preparation:\n",
    "1. The entire dataset consists of `string` types, which, by the nature of the dataset, should not be the case. We need to convert the data into appropriate types.\n",
    "2. The date-related attributes are split into their parts (`Year`, `Month`, ...). It is best that we store it into a single date column instead to allow for easier sorting of the dataframe. \n",
    "3. The `Amount` column is represented with the literal numerical value, which contains the dolar sign and other artifacts. This also needs to be fixed such that it can be represented as a `double` type.\n",
    "4. We can also normalize the column names for easier manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the column names:\n",
    "for col in df_transactions.columns:\n",
    "    df_transactions = df_transactions.withColumnRenamed(\n",
    "        col, col.lower().strip().replace(\" \", \"_\").replace(\"?\", \"\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "card\n",
      "year\n",
      "month\n",
      "day\n",
      "time\n",
      "amount\n",
      "use_chip\n",
      "merchant_name\n",
      "merchant_city\n",
      "merchant_state\n",
      "zip\n",
      "mcc\n",
      "errors\n",
      "is_fraud\n"
     ]
    }
   ],
   "source": [
    "# columns normalized become:\n",
    "for col in df_transactions.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Converting Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can remove the first digit and the convert to a double that should work for fixing this column\n",
    "df_transactions = df_transactions.withColumn(\n",
    "    \"amount\", F.expr(\"substring(amount, 2, length(amount))\").cast(\"double\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Year, month, day, time\n",
    "To improve the date-based columns, we have to do a few things first:\n",
    "\n",
    "1. Pad the digits to strings of length 2 when appropriate;\n",
    "2. Concatenate the strings to a single value;\n",
    "3. Convert the value to a `timestamp` object;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the month and day columns:\n",
    "df_transactions = df_transactions.withColumn(\"month\", F.lpad(F.col(\"month\"), 2, \"0\"))\n",
    "\n",
    "df_transactions = df_transactions.withColumn(\"day\", F.lpad(F.col(\"day\"), 2, \"0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the right side of the time column\n",
    "df_transactions = df_transactions.withColumn(\"time\", F.rpad(F.col(\"time\"), 8, \":00\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating the columns:\n",
    "df_transactions = df_transactions.withColumn(\n",
    "    \"timestamp\",\n",
    "    F.concat(\n",
    "        F.col(\"year\"),\n",
    "        F.lit(\"-\"),\n",
    "        F.col(\"month\"),\n",
    "        F.lit(\"-\"),\n",
    "        F.col(\"day\"),\n",
    "        F.lit(\" \"),\n",
    "        F.col(\"time\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "df_transactions = df_transactions.withColumn(\n",
    "    \"timestamp\", F.to_timestamp(F.col(\"timestamp\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Fraud Indicator\n",
    "We will convert this column to a boolean as it is essentially a binary indicator for fraud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = df_transactions.withColumn(\n",
    "    \"is_fraud\", F.when(F.col(\"is_fraud\") == \"Yes\", True).otherwise(False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Zip codes\n",
    "We will fix the zip codes by setting them to integers and then padding zeros to the left when appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting zip codes to integers:\n",
    "df_transactions = df_transactions.withColumn(\n",
    "    \"zip\", F.col(\"zip\").cast(\"integer\")\n",
    ").withColumn(\"zip\", F.col(\"zip\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left-padding the zipcodes\n",
    "df_transactions = df_transactions.withColumn(\"zip\", F.lpad(F.col(\"zip\"), 5, \"0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preparing the Preprocessed output\n",
    "For further use of the dataset, I will drop the columns I won't be needing for the remainder of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = df_transactions.select(\n",
    "    F.col(\"user\").alias(\"user_id\"),\n",
    "    F.col(\"card\").alias(\"card_id\"),\n",
    "    F.col(\"timestamp\").alias(\"transaction_timestamp\"),\n",
    "    F.col(\"amount\").alias(\"transaction_amount\"),\n",
    "    F.col(\"use_chip\").alias(\"transaction_type\"),\n",
    "    F.col(\"mcc\").alias(\"merchant_category_code\"),\n",
    "    F.col(\"merchant_name\"),\n",
    "    F.col(\"merchant_city\"),\n",
    "    F.col(\"merchant_state\"),\n",
    "    F.col(\"zip\").alias(\"zip_code\"),\n",
    "    F.col(\"is_fraud\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the lookup table defined:\n",
    "with open(RAW_DATA_DIR + \"mcc_lookup.json\", \"r\") as file:\n",
    "    mcc_list = json.load(file)\n",
    "\n",
    "lookup = {}\n",
    "for item in mcc_list:\n",
    "    mcc = item[\"mcc\"]\n",
    "    desc = item[\"description\"]\n",
    "\n",
    "    lookup[mcc] = desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's apply a hash map to the mcc column given the description set we have defined above\n",
    "df_transactions = df_transactions.withColumn(\n",
    "    \"merchant_category_description\",\n",
    "    apply_category_map(lookup)(F.col(\"merchant_category_code\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the intermediary preprocessed output:\n",
    "PROCESSED_DATA_DIR = \"../data/processed/\"\n",
    "\n",
    "save_to_filesystem(\n",
    "    df_transactions, PROCESSED_DATA_DIR, \"tb_transactions\", \"tb_transactions.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Converting the Dataset into Timeline Format\n",
    "For this project, I will study the sequence of transactions an user makes with their credit card. With that, we have interest in making this dataset work in a sequential matter. This can be accomplished by writing out the sequence based on the `transaction_timestamp`, with a minimum unit of a `day`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a few attributes about the time of the day\n",
    "df_timeline = df_transactions.withColumn(\n",
    "    \"time_of_day\",\n",
    "    F.when((F.hour(F.col(\"transaction_timestamp\")) < 6), \"Early Morning\")\n",
    "    .when(\n",
    "        (F.hour(F.col(\"transaction_timestamp\")) >= 6)\n",
    "        & (F.hour(F.col(\"transaction_timestamp\")) < 13),\n",
    "        \"Morning\",\n",
    "    )\n",
    "    .when(\n",
    "        (F.hour(F.col(\"transaction_timestamp\")) >= 13)\n",
    "        & (F.hour(F.col(\"transaction_timestamp\")) < 18),\n",
    "        \"Afternoon\",\n",
    "    )\n",
    "    .otherwise(\"Night\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a helper column to the sequence:\n",
    "df_timeline = df_timeline.withColumn(\n",
    "    \"transaction_date\", F.to_date(F.col(\"transaction_timestamp\"))\n",
    ")\n",
    "\n",
    "\n",
    "timeline = Window.partitionBy(\"user_id\", \"transaction_date\").orderBy(\n",
    "    \"transaction_timestamp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the sequence of events:\n",
    "df_timeline = df_timeline.withColumn(\"transaction_order\", F.row_number().over(timeline))\n",
    "\n",
    "# generating the next element for the sequence as a helper column\n",
    "df_timeline = df_timeline.withColumn(\n",
    "    \"timestamp_last_transaction\",\n",
    "    F.lag(F.col(\"transaction_timestamp\"), 1).over(timeline),\n",
    ")\n",
    "\n",
    "# adding a boolean handle for the first transaction in the dataset\n",
    "df_timeline = df_timeline.withColumn(\n",
    "    \"is_first_transaction_of_day\", (F.col(\"timestamp_last_transaction\").isNull())\n",
    ")\n",
    "\n",
    "# generating the time between events:\n",
    "df_timeline = df_timeline.withColumn(\n",
    "    \"time_between_purchases_in_seconds\",\n",
    "    F.when(F.col(\"timestamp_last_transaction\").isNull(), None).otherwise(\n",
    "        F.floor(\n",
    "            (\n",
    "                F.col(\"transaction_timestamp\").cast(\"long\")\n",
    "                - F.col(\"timestamp_last_transaction\").cast(\"long\")\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding time-based features about the day:\n",
    "df_timeline = get_datetime_features(df_timeline, \"transaction_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping helper columns:\n",
    "df_timeline = df_timeline.drop(\n",
    "    \"timestamp_last_transaction\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>card_id</th><th>transaction_timestamp</th><th>transaction_amount</th><th>transaction_type</th><th>merchant_category_code</th><th>merchant_name</th><th>merchant_city</th><th>merchant_state</th><th>zip_code</th><th>is_fraud</th><th>merchant_category_description</th><th>time_of_day</th><th>transaction_date</th><th>transaction_order</th><th>is_first_transaction_of_day</th><th>time_between_purchases_in_seconds</th><th>day_of_week</th><th>day_of_month</th><th>day_of_year</th><th>week_of_year</th><th>month</th><th>quarter</th><th>year</th></tr>\n",
       "<tr><td>0</td><td>0</td><td>2002-09-07 06:16:00</td><td>117.05</td><td>Swipe Transaction</td><td>5411</td><td>-727612092139916043</td><td>Monterey Park</td><td>CA</td><td>91754</td><td>false</td><td>Groceries and sup...</td><td>Morning</td><td>2002-09-07</td><td>1</td><td>true</td><td>null</td><td>7</td><td>7</td><td>250</td><td>36</td><td>9</td><td>3</td><td>2002</td></tr>\n",
       "<tr><td>0</td><td>0</td><td>2002-09-07 06:34:00</td><td>45.3</td><td>Swipe Transaction</td><td>5942</td><td>-5475680618560174533</td><td>Monterey Park</td><td>CA</td><td>91755</td><td>false</td><td>Bookshops</td><td>Morning</td><td>2002-09-07</td><td>2</td><td>false</td><td>1080</td><td>7</td><td>7</td><td>250</td><td>36</td><td>9</td><td>3</td><td>2002</td></tr>\n",
       "<tr><td>0</td><td>0</td><td>2002-09-07 09:39:00</td><td>29.34</td><td>Swipe Transaction</td><td>7538</td><td>4055257078481058705</td><td>La Verne</td><td>CA</td><td>91750</td><td>false</td><td>Automotive servic...</td><td>Morning</td><td>2002-09-07</td><td>3</td><td>false</td><td>11100</td><td>7</td><td>7</td><td>250</td><td>36</td><td>9</td><td>3</td><td>2002</td></tr>\n",
       "<tr><td>0</td><td>0</td><td>2002-11-03 05:36:00</td><td>150.08</td><td>Swipe Transaction</td><td>5300</td><td>3527213246127876953</td><td>La Verne</td><td>CA</td><td>91750</td><td>false</td><td>Wholesale clubs</td><td>Early Morning</td><td>2002-11-03</td><td>1</td><td>true</td><td>null</td><td>1</td><td>3</td><td>307</td><td>44</td><td>11</td><td>4</td><td>2002</td></tr>\n",
       "<tr><td>0</td><td>0</td><td>2002-11-03 06:05:00</td><td>180.58</td><td>Swipe Transaction</td><td>5912</td><td>5817218446178736267</td><td>La Verne</td><td>CA</td><td>91750</td><td>false</td><td>Drug stores and p...</td><td>Morning</td><td>2002-11-03</td><td>2</td><td>false</td><td>1740</td><td>1</td><td>3</td><td>307</td><td>44</td><td>11</td><td>4</td><td>2002</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+-------+-------+---------------------+------------------+-----------------+----------------------+--------------------+-------------+--------------+--------+--------+-----------------------------+-------------+----------------+-----------------+---------------------------+---------------------------------+-----------+------------+-----------+------------+-----+-------+----+\n",
       "|user_id|card_id|transaction_timestamp|transaction_amount| transaction_type|merchant_category_code|       merchant_name|merchant_city|merchant_state|zip_code|is_fraud|merchant_category_description|  time_of_day|transaction_date|transaction_order|is_first_transaction_of_day|time_between_purchases_in_seconds|day_of_week|day_of_month|day_of_year|week_of_year|month|quarter|year|\n",
       "+-------+-------+---------------------+------------------+-----------------+----------------------+--------------------+-------------+--------------+--------+--------+-----------------------------+-------------+----------------+-----------------+---------------------------+---------------------------------+-----------+------------+-----------+------------+-----+-------+----+\n",
       "|      0|      0|  2002-09-07 06:16:00|            117.05|Swipe Transaction|                  5411| -727612092139916043|Monterey Park|            CA|   91754|   false|         Groceries and sup...|      Morning|      2002-09-07|                1|                       true|                             null|          7|           7|        250|          36|    9|      3|2002|\n",
       "|      0|      0|  2002-09-07 06:34:00|              45.3|Swipe Transaction|                  5942|-5475680618560174533|Monterey Park|            CA|   91755|   false|                    Bookshops|      Morning|      2002-09-07|                2|                      false|                             1080|          7|           7|        250|          36|    9|      3|2002|\n",
       "|      0|      0|  2002-09-07 09:39:00|             29.34|Swipe Transaction|                  7538| 4055257078481058705|     La Verne|            CA|   91750|   false|         Automotive servic...|      Morning|      2002-09-07|                3|                      false|                            11100|          7|           7|        250|          36|    9|      3|2002|\n",
       "|      0|      0|  2002-11-03 05:36:00|            150.08|Swipe Transaction|                  5300| 3527213246127876953|     La Verne|            CA|   91750|   false|              Wholesale clubs|Early Morning|      2002-11-03|                1|                       true|                             null|          1|           3|        307|          44|   11|      4|2002|\n",
       "|      0|      0|  2002-11-03 06:05:00|            180.58|Swipe Transaction|                  5912| 5817218446178736267|     La Verne|            CA|   91750|   false|         Drug stores and p...|      Morning|      2002-11-03|                2|                      false|                             1740|          1|           3|        307|          44|   11|      4|2002|\n",
       "+-------+-------+---------------------+------------------+-----------------+----------------------+--------------------+-------------+--------------+--------+--------+-----------------------------+-------------+----------------+-----------------+---------------------------+---------------------------------+-----------+------------+-----------+------------+-----+-------+----+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying the results:\n",
    "df_timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROCESSED_DATA_DIR = \"../data/processed/\"\n",
    "\n",
    "save_to_filesystem(\n",
    "    df_timeline, PROCESSED_DATA_DIR, \"tb_timeline\", \"tb_timeline.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Representing Sequences of transactions\n",
    "We have many ways of representing the sequences of transactions a customer makes. We can consider then by day, for example. We could also consider them at an user level (that is, the all time sequence related to a single customer). \n",
    "\n",
    "I will generate datasets for each of these representations, but also generate one that is a compound feature of both cases. I will first gather sequences by them, only to gather them by user. These three structures might help us uncover different kinds of patterns in further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the timeline from cache:\n",
    "PROCESSED_DATA_DIR = \"../data/processed/\"\n",
    "\n",
    "df_timeline = spark.read.parquet(PROCESSED_DATA_DIR + \"tb_timeline.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the representations or the order at user, date level\n",
    "df_seq = df_timeline.groupby(\"user_id\", \"transaction_date\").agg(\n",
    "    F.collect_list(\"merchant_category_code\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>transaction_date</th><th>collect_list(merchant_category_code)</th></tr>\n",
       "<tr><td>0</td><td>2002-09-07</td><td>[5411, 5942, 7538]</td></tr>\n",
       "<tr><td>0</td><td>2002-11-03</td><td>[5300, 5912]</td></tr>\n",
       "<tr><td>0</td><td>2004-11-10</td><td>[5300]</td></tr>\n",
       "<tr><td>0</td><td>2005-06-17</td><td>[5912, 5411, 5651...</td></tr>\n",
       "<tr><td>0</td><td>2005-08-20</td><td>[4829, 5912, 5815...</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+-------+----------------+------------------------------------+\n",
       "|user_id|transaction_date|collect_list(merchant_category_code)|\n",
       "+-------+----------------+------------------------------------+\n",
       "|      0|      2002-09-07|                  [5411, 5942, 7538]|\n",
       "|      0|      2002-11-03|                        [5300, 5912]|\n",
       "|      0|      2004-11-10|                              [5300]|\n",
       "|      0|      2005-06-17|                [5912, 5411, 5651...|\n",
       "|      0|      2005-08-20|                [4829, 5912, 5815...|\n",
       "+-------+----------------+------------------------------------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the sequence can be verified as follows:\n",
    "df_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MCCs, themselves, however, are not easily interpretable. They require the understanding of the **ISO 18245** specification (which by itself is not that easy to interpret sometimes). The codes effectively represent *\"ideas\"* or *\"situations\"* regarding the purchase the client made. It encapsulates both a kind of merchant and what the merchant is allowed to sell (ISO 18245 is used to regulate what fees merchants would pay to credit card processors as part of the transactions, for example).\n",
    "\n",
    "What we can do here is to convert these series of *situations* into something more meaningful and easier to interpret both by humans and possibly by any algorithms we use to extract informatio about them. This is quite similar to the way `ideograms` work and an easily interpretable way to convert text or code into an ideogram is to use **emojis**.\n",
    "\n",
    "Emojis are both unique **unicode-complaint** characters, but also can be interpreted visually. It can be used as a surrogate representation of the merchant category codes and allow us to apply *standard text processing techniques*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the mcc to emoji hash table and mcc to description:\n",
    "with open(RAW_DATA_DIR + \"mcc_encoding.json\", \"r\") as file:\n",
    "    mcc_encoding = json.load(file)\n",
    "\n",
    "with open(RAW_DATA_DIR + \"mcc_description.json\", \"r\") as file:\n",
    "    mcc_description = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving the dictionary of emojis:\n",
    "emojis = emoji.EMOJI_ALIAS_UNICODE_ENGLISH\n",
    "\n",
    "# doing some preprocessing to the lookup:\n",
    "emojis_clean = {key.lower(): val for key, val in emojis.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's generate the mapping in terms of emojis:\n",
    "mcc_emoji = {}\n",
    "\n",
    "for mcc, shortcut in mcc_encoding.items():\n",
    "    try:\n",
    "        mcc_emoji[mcc] = emojis_clean[shortcut]\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1711 -> ğŸ”¨\n",
      "3000 -> âœˆ\n",
      "3001 -> âœˆ\n",
      "3005 -> âœˆ\n",
      "3006 -> âœˆ\n",
      "3007 -> âœˆ\n",
      "3008 -> âœˆ\n",
      "3009 -> âœˆ\n",
      "3058 -> âœˆ\n",
      "3066 -> âœˆ\n"
     ]
    }
   ],
   "source": [
    "# visualizing a few of the encodings:\n",
    "for mcc, emoji in list(mcc_emoji.items())[:10]:\n",
    "    print(f\"{mcc} -> {emoji}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the mapping to the timeline dataframe:\n",
    "df_timeline = df_timeline.withColumn(\n",
    "    \"merchant_category_encoding\",\n",
    "    apply_category_map(mcc_emoji)(F.col(\"merchant_category_code\")),\n",
    ")\n",
    "\n",
    "df_timeline = df_timeline.withColumn(\n",
    "    \"merchant_category_description\",\n",
    "    apply_category_map(mcc_description)(F.col(\"merchant_category_code\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_category_code</th><th>merchant_category_description</th><th>merchant_category_encoding</th></tr>\n",
       "<tr><td>4214</td><td>Motor freight car...</td><td>ğŸšš</td></tr>\n",
       "<tr><td>5192</td><td>Books, periodical...</td><td>ğŸ“š</td></tr>\n",
       "<tr><td>7393</td><td>Detective agencie...</td><td>ğŸ‘®</td></tr>\n",
       "<tr><td>7549</td><td>Towing services</td><td>ğŸš—</td></tr>\n",
       "<tr><td>3780</td><td>Lodging â€” hotels,...</td><td>ğŸ¨</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+----------------------+-----------------------------+--------------------------+\n",
       "|merchant_category_code|merchant_category_description|merchant_category_encoding|\n",
       "+----------------------+-----------------------------+--------------------------+\n",
       "|                  4214|         Motor freight car...|                        ğŸšš|\n",
       "|                  5192|         Books, periodical...|                        ğŸ“š|\n",
       "|                  7393|         Detective agencie...|                        ğŸ‘®|\n",
       "|                  7549|              Towing services|                        ğŸš—|\n",
       "|                  3780|         Lodging â€” hotels,...|                        ğŸ¨|\n",
       "+----------------------+-----------------------------+--------------------------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can then visualize some of these ideas represented by the emoji's relation to the MCCs:\n",
    "df_timeline.select(\n",
    "    \"merchant_category_code\",\n",
    "    \"merchant_category_description\",\n",
    "    \"merchant_category_encoding\",\n",
    ").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>merchant_category_code</th><th>transaction_amount</th><th>merchant_category_encoding</th></tr>\n",
       "<tr><td>0</td><td>5411</td><td>117.05</td><td>ğŸ›’</td></tr>\n",
       "<tr><td>0</td><td>5942</td><td>45.3</td><td>ğŸ“š</td></tr>\n",
       "<tr><td>0</td><td>7538</td><td>29.34</td><td>ğŸš—</td></tr>\n",
       "<tr><td>0</td><td>5300</td><td>150.08</td><td>ğŸ¬</td></tr>\n",
       "<tr><td>0</td><td>5912</td><td>180.58</td><td>ğŸ’Š</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+-------+----------------------+------------------+--------------------------+\n",
       "|user_id|merchant_category_code|transaction_amount|merchant_category_encoding|\n",
       "+-------+----------------------+------------------+--------------------------+\n",
       "|      0|                  5411|            117.05|                        ğŸ›’|\n",
       "|      0|                  5942|              45.3|                        ğŸ“š|\n",
       "|      0|                  7538|             29.34|                        ğŸš—|\n",
       "|      0|                  5300|            150.08|                        ğŸ¬|\n",
       "|      0|                  5912|            180.58|                        ğŸ’Š|\n",
       "+-------+----------------------+------------------+--------------------------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# putting the results into perspective for a single customer:\n",
    "df_timeline.select(\n",
    "    \"user_id\",\n",
    "    \"merchant_category_code\",\n",
    "    \"transaction_amount\",\n",
    "    \"merchant_category_encoding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Building Sequence Datasets\n",
    "In order to appropriately analyze the customer transactions from a sequence perspective, we will need to generate a few different views of the timeline dataset. These are:\n",
    "\n",
    "1. The entire transactional history for each customer;\n",
    "2. The daily transactional history for each customer;\n",
    "3. The entire transactional history for each customer but maintaining the scope of daily sequences;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 User Transactional Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 User Daily Transactional Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user level representation:\n",
    "df_daily_history = df_timeline.groupby(\"user_id\", \"transaction_date\").agg(\n",
    "    F.concat_ws(\"\", F.collect_list(\"merchant_category_encoding\")).alias(\"sequence\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>transaction_date</th><th>sequence</th></tr>\n",
       "<tr><td>0</td><td>2002-09-07</td><td>ğŸ›’ğŸ“šğŸš—</td></tr>\n",
       "<tr><td>0</td><td>2002-11-03</td><td>ğŸ¬ğŸ’Š</td></tr>\n",
       "<tr><td>0</td><td>2004-11-10</td><td>ğŸ¬</td></tr>\n",
       "<tr><td>0</td><td>2005-06-17</td><td>ğŸ’ŠğŸ›’ğŸ‘•ğŸ’Š</td></tr>\n",
       "<tr><td>0</td><td>2005-08-20</td><td>ğŸ’°ğŸ’ŠğŸ“šğŸ“±</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+-------+----------------+--------+\n",
       "|user_id|transaction_date|sequence|\n",
       "+-------+----------------+--------+\n",
       "|      0|      2002-09-07|  ğŸ›’ğŸ“šğŸš—|\n",
       "|      0|      2002-11-03|    ğŸ¬ğŸ’Š|\n",
       "|      0|      2004-11-10|      ğŸ¬|\n",
       "|      0|      2005-06-17|ğŸ’ŠğŸ›’ğŸ‘•ğŸ’Š|\n",
       "|      0|      2005-08-20|ğŸ’°ğŸ’ŠğŸ“šğŸ“±|\n",
       "+-------+----------------+--------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying the results:\n",
    "df_daily_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the nature of the dataset (being synthetic), it is prone to some errors or some types of biases. One of the problems this specific dataset has is to duplicate sequential purchases of the same MCC. This is unlikely to happen in the real world, so we need to perform some data cleaning on those cases. For our purposes I will remove the duplicate entries that happen sequentially from the sequences, using the `remove_duplicate_elements` helper UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the cleaning function to the sequence column:\n",
    "df_daily_history = df_daily_history.withColumn(\n",
    "    \"sequence\", remove_duplicate_elements(F.col(\"sequence\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_to_filesystem(\n",
    "    df_daily_history,\n",
    "    PROCESSED_DATA_DIR,\n",
    "    \"tb_user_daily_sequence\",\n",
    "    \"tb_user_daily_sequence.parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 User Transactional History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's combine both the daily view and the\n",
    "df_user_history = df_timeline.groupby(\"user_id\").agg(\n",
    "    F.concat_ws(\",\", F.collect_list(\"merchant_category_encoding\")).alias(\"sequence\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>sequence</th></tr>\n",
       "<tr><td>1090</td><td>â›½,ğŸ¬,ğŸ’°,ğŸ›’,ğŸ›’,ğŸ’°,...</td></tr>\n",
       "<tr><td>1159</td><td>ğŸ“±,ğŸ›’,ğŸ½,ğŸª‘,ğŸ’,ğŸ½...</td></tr>\n",
       "<tr><td>1436</td><td>ğŸ½,ğŸ’Š,ğŸ¬,ğŸ¬,ğŸ¨,ğŸ¬...</td></tr>\n",
       "<tr><td>1512</td><td>ğŸ¬,ğŸ’Š,ğŸ ,ğŸ§¾,ğŸ§¾,ğŸ›’...</td></tr>\n",
       "<tr><td>1572</td><td>ğŸ›’,ğŸ›’,ğŸ›’,ğŸ½,ğŸ›’,ğŸŒ¯...</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+-------+--------------------+\n",
       "|user_id|            sequence|\n",
       "+-------+--------------------+\n",
       "|   1090|â›½,ğŸ¬,ğŸš•,ğŸ’°,ğŸ¬,ğŸ’°,...|\n",
       "|   1159|ğŸ›’,ğŸ½,ğŸ¬,ğŸ›’,ğŸ½,ğŸ½...|\n",
       "|   1436|ğŸ½,ğŸ›’,ğŸ¬,â›½,ğŸ¨,ğŸ›’,...|\n",
       "|   1512|ğŸ’Š,ğŸ¬,ğŸ§¾,ğŸ ,ğŸ›’,ğŸ§¾...|\n",
       "|   1572|ğŸ›’,ğŸ’ˆ,ğŸ½,ğŸ¬,ğŸŒ¯,ğŸ’°...|\n",
       "+-------+--------------------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intermediary results become:\n",
    "df_user_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_to_filesystem(\n",
    "    df_user_history,\n",
    "    PROCESSED_DATA_DIR,\n",
    "    \"tb_user_history\",\n",
    "    \"tb_user_history.parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Tokenized User Transacional History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping the daily sequences into user sequences by separator:\n",
    "df_tokens = df_daily_history.groupby(\"user_id\").agg(\n",
    "    F.concat_ws(\",\", F.collect_list(F.col(\"sequence\"))).alias(\"sequence\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>sequence</th></tr>\n",
       "<tr><td>0</td><td>ğŸ›’ğŸ“šğŸš—,ğŸ¬ğŸ’Š,ğŸ¬,ğŸ’Š...</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+--------------------+\n",
       "|user_id|            sequence|\n",
       "+-------+--------------------+\n",
       "|      0|ğŸ›’ğŸ“šğŸš—,ğŸ¬ğŸ’Š,ğŸ¬,ğŸ’Š...|\n",
       "+-------+--------------------+"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the final results can be illustrated as:\n",
    "df_tokens.filter(F.col(\"user_id\") == \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_to_filesystem(\n",
    "    df_tokens,\n",
    "    PROCESSED_DATA_DIR,\n",
    "    \"tb_user_token_history\",\n",
    "    \"tb_user_token_history.parquet\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
